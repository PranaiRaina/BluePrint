from RAG_PIPELINE.src.graph import app_graph
import asyncio
import os


async def perform_rag_search(
    query: str, user_id: str = "fallback-user-id", session_id: str = "default"
) -> str:
    """
    Search connected documents for answers using the RAG pipeline.

    Args:
        query: The user's question about the documents (e.g., "Summarize the invoice").
        user_id: The unique ID of the user to scope the search to.
        session_id: The session ID for LangGraph persistence.

    Returns:
        The text answer generated by the document analysis system.
    """
    try:
        # Invoke the LangGraph pipeline
        # Pass the session_id as the thread_id for persistence
        result = await app_graph.ainvoke(
            {"question": query, "user_id": user_id},
            config={"configurable": {"thread_id": session_id}},
        )

        # Extract the generation
        answer = result.get("generation")
        if not answer:
            return "Analysis complete, but no direct answer was generated."

        return answer
    except Exception as e:
        return f"Error performing RAG search: {str(e)}"


async def perform_rag_search_stream(
    query: str, user_id: str = "fallback-user-id", session_id: str = "default"
):
    """
    Streamed version of perform_rag_search.
    Yields:
        Reading: {"type": "status", "content": "..."}
        Tokens:  {"type": "token", "content": "..."}
    """
    try:
        # Use astream_events to capture the LLM output from the 'generate' node
        # Pass the session_id as the thread_id for persistence
        async for event in app_graph.astream_events(
            {"question": query, "user_id": user_id},
            config={"configurable": {"thread_id": session_id}},
            version="v2"
        ):
            kind = event["event"]

            if kind == "on_chat_model_stream":
                # Robust filtering: Check for tag OR node name
                is_final = "final_generation" in event.get("tags", [])
                is_gen_node = event.get("metadata", {}).get("langgraph_node") == "generate"

                if is_final or is_gen_node:
                    chunk = event["data"]["chunk"]
                    content = chunk.content if hasattr(chunk, "content") else str(chunk)
                    if content:
                        yield {"type": "token", "content": content}
                        await asyncio.sleep(0)

            elif kind == "on_chain_end":
                # Capture citations when the 'generate' node finishes
                if event.get("name") == "generate":
                    output = event.get("data", {}).get("output", {})
                    docs = output.get("documents", [])
                    if docs:
                        # Format unique sources
                        sources = set()
                        for d in docs:
                            source = d.metadata.get("source", "Unknown")
                            if source != "Tavily Search": 
                                sources.add(os.path.basename(str(source)))
                            else:
                                sources.add("Web Search")

                        if sources:
                            source_msg = (
                                f"\n\n*Sources: {', '.join(sorted(list(sources)))}*"
                            )
                            yield {"type": "token", "content": source_msg}

            elif kind == "on_tool_start":
                # Yield status for internal RAG tool usage
                tool_name = event["name"]
                display_name = "Processing..."

                if "tavily" in tool_name.lower():
                    display_name = "Searching Web..."
                elif "retriev" in tool_name.lower():
                    display_name = "Reading Documents..."
                else:
                    display_name = f"Using {tool_name}..."

                yield {"type": "status", "content": display_name}

    except Exception as e:
        yield {"type": "token", "content": f"Error in RAG stream: {str(e)}"}


async def ask_stock_analyst(query: str) -> str:
    """
    Delegate a stock analysis question to the Stock Analysis Multi-Agent System.
    Use this for any question about stock prices, market trends, portfolio optimization,
    or investment recommendations for specific tickers (e.g., AAPL, TSLA, NVDA).

    Args:
        query: The user's question about stocks or market analysis.

    Returns:
        A comprehensive analysis and recommendation from the stock analysis team.
    """
    try:
        from StockAgents.services.agent_engine import agent_engine
        from StockAgents.services.llm_service import llm_service

        # Extract user's portfolio context from query (e.g., "I have 50 shares of AAPL")
        user_context = await llm_service.extract_structured_data(query)
        print(f"[ask_stock_analyst] Extracted user_context: {user_context}")

        # Run the multi-agent workflow with the extracted context
        result = await agent_engine.run_workflow(query, user_context=user_context)

        # Return the synthesized recommendation
        return result.get(
            "recommendation",
            "Stock analysis completed, but no recommendation was generated.",
        )
    except Exception as e:
        return f"Error performing stock analysis: {str(e)}"


async def ask_stock_analyst_stream(query: str):
    """
    Streamed version of ask_stock_analyst.
    """
    try:
        from StockAgents.services.agent_engine import agent_engine
        from StockAgents.services.llm_service import llm_service

        yield {"type": "status", "content": "Analyzing Request..."}
        user_context = await llm_service.extract_structured_data(query)

        # Run workflow stream
        async for chunk in agent_engine.run_workflow_stream(
            query, user_context=user_context
        ):
            yield chunk

    except Exception as e:
        yield {"type": "token", "content": f"Error in Stock stream: {str(e)}"}
