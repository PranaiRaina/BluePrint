from RAG_PIPELINE.src.graph import app_graph
import asyncio


async def perform_rag_search(query: str, user_id: str = "fallback-user-id") -> str:
    """
    Search connected documents for answers using the RAG pipeline.

    Args:
        query: The user's question about the documents (e.g., "Summarize the invoice").
        user_id: The unique ID of the user to scope the search to.

    Returns:
        The text answer generated by the document analysis system.
    """
    try:
        # Invoke the LangGraph pipeline
        # The graph now expects user_id for filtering
        result = await app_graph.ainvoke({"question": query, "user_id": user_id})

        # Extract the generation
        answer = result.get("generation")
        if not answer:
            return "Analysis complete, but no direct answer was generated."

        return answer
    except Exception as e:
        return f"Error performing RAG search: {str(e)}"


async def perform_rag_search_stream(query: str, user_id: str = "fallback-user-id"):
    """
    Streamed version of perform_rag_search.
    Yields:
        Reading: {"type": "status", "content": "..."}
        Tokens:  {"type": "token", "content": "..."}
    """
    try:
        # Use astream_events to capture the LLM output from the 'generate' node
        async for event in app_graph.astream_events(
            {"question": query, "user_id": user_id}, version="v2"
        ):
            kind = event["event"]

            if kind == "on_chat_model_stream":
                # Check if this stream is coming from the 'generate' node
                # The 'generate' node uses 'ChatGoogleGenerativeAI'
                # In V2, we get standard Run events
                chunk = event["data"]["chunk"]
                content = chunk.content if hasattr(chunk, "content") else str(chunk)
                if content:
                    yield {"type": "token", "content": content}
                    await asyncio.sleep(0)  # Force buffer flush

            elif kind == "on_tool_start":
                # Yield status for internal RAG tool usage
                tool_name = event["name"]
                display_name = "Processing..."

                if "tavily" in tool_name.lower():
                    display_name = "Searching Web..."
                elif "retriev" in tool_name.lower():
                    display_name = "Reading Documents..."
                else:
                    display_name = f"Using {tool_name}..."

                yield {"type": "status", "content": display_name}

    except Exception as e:
        yield {"type": "token", "content": f"Error in RAG stream: {str(e)}"}


async def ask_stock_analyst(query: str) -> str:
    """
    Delegate a stock analysis question to the Stock Analysis Multi-Agent System.
    Use this for any question about stock prices, market trends, portfolio optimization,
    or investment recommendations for specific tickers (e.g., AAPL, TSLA, NVDA).

    Args:
        query: The user's question about stocks or market analysis.

    Returns:
        A comprehensive analysis and recommendation from the stock analysis team.
    """
    try:
        from StockAgents.services.agent_engine import agent_engine
        from StockAgents.services.llm_service import llm_service

        # Extract user's portfolio context from query (e.g., "I have 50 shares of AAPL")
        user_context = await llm_service.extract_structured_data(query)
        print(f"[ask_stock_analyst] Extracted user_context: {user_context}")

        # Run the multi-agent workflow with the extracted context
        result = await agent_engine.run_workflow(query, user_context=user_context)

        # Return the synthesized recommendation
        return result.get(
            "recommendation",
            "Stock analysis completed, but no recommendation was generated.",
        )
    except Exception as e:
        return f"Error performing stock analysis: {str(e)}"


async def ask_stock_analyst_stream(query: str):
    """
    Streamed version of ask_stock_analyst.
    """
    try:
        from StockAgents.services.agent_engine import agent_engine
        from StockAgents.services.llm_service import llm_service

        yield {"type": "status", "content": "Analyzing Request..."}
        user_context = await llm_service.extract_structured_data(query)

        # Run workflow stream
        async for chunk in agent_engine.run_workflow_stream(
            query, user_context=user_context
        ):
            yield chunk

    except Exception as e:
        yield {"type": "token", "content": f"Error in Stock stream: {str(e)}"}
